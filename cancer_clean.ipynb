{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475adce1-99e9-4773-a2ef-eb45e8c5dcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset,  Dataset\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745b68e-1d86-47c4-a4a4-19d70e6d5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the Base Model and Tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # Use FP32 for CPU\n",
    "    device_map=\"cpu\"     # Run on CPU\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88881753-464d-436f-867b-c7b5b6d45521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the Model for LoRA Fine-Tuning\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c8d44-a530-4506-8eb8-a17baec3ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying potential modules for use in Lora configuration (useful if you don't know the modules in model, run list through chatgpt to find out which ones to use in your model)\n",
    "for name, module in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037db6da-ffc0-4a2e-842f-1d65f03e76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],  # Layers to adapt, identified in the optional step above\n",
    "    lora_dropout=0.1,  # Dropout for LoRA\n",
    "    bias=\"none\",  # Do not fine-tune biases\n",
    "    task_type=\"CAUSAL_LM\"  # Task type: causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the Model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14744a2c-b407-4c5f-ab02-d3ba0680a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load and Prepare the Datasets\n",
    "# Load the question-answer dataset\n",
    "import json\n",
    "with open(\"cancer_qa.txt\", \"r\") as f:\n",
    "    qa_data = json.load(f)\n",
    "\n",
    "qa_dataset = Dataset.from_dict({\n",
    "    \"prompt\": [item[\"question\"] for item in qa_data],\n",
    "    \"response\": [item[\"answer\"] for item in qa_data]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c245cc-fde3-4d5e-93c6-69714d53c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unstructured course notes\n",
    "with open(\"cancer_data.txt\", \"r\") as f:\n",
    "    course_notes = f.readlines()\n",
    "\n",
    "# Create synthetic prompt-response pairs from course notes\n",
    "unstructured_data = [\n",
    "    {\"prompt\": f\"Explain: {note.strip()}\", \"response\": note.strip()} for note in course_notes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faabd3-d9c9-4e97-8127-34a3ab987ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "all_data = qa_dataset.add_batch(\n",
    "    {\"prompt\": [item[\"prompt\"] for item in unstructured_data], \n",
    "     \"response\": [item[\"response\"] for item in unstructured_data]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073277cc-a500-4f62-a6b2-547db57e488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    # Concatenate prompt and response element-wise\n",
    "    combined_texts = [p + \" \" + r for p, r in zip(examples[\"prompt\"], examples[\"response\"])]\n",
    "    # Tokenize the concatenated texts\n",
    "    inputs = tokenizer(combined_texts, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()  # Set input_ids as labels\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9687746-2f0f-4665-8b8e-778de9d4cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing to the dataset\n",
    "tokenized_dataset = all_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3025504d-9061-4d01-8de6-736e1e9e5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen2.5-c-tuned\",  # Directory to save the fine-tuned model\n",
    "    per_device_train_batch_size=1,  # Batch size for CPU\n",
    "    num_train_epochs=3,  # Number of epochs\n",
    "    save_steps=500,  # Save checkpoint every 500 steps\n",
    "    logging_dir=\"./logs\",  # Log directory\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    evaluation_strategy=\"no\",  # Disable evaluation (CPU performance optimization)\n",
    "    fp16=False,  # Disable mixed precision (CPU only)\n",
    "    push_to_hub=False  # Do not push to Hugging Face Hub\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b8cbf-620a-415a-a493-b5a85ef38ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the Model\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to train\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=tokenized_dataset,  # The tokenized dataset\n",
    "    tokenizer=tokenizer  # Tokenizer for preprocessing\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#TrainOutput(global_step=249, training_loss=1.2162665707998008, metrics={'train_runtime': 7465.7666, 'train_samples_per_second': 0.033, 'train_steps_per_second': 0.033, 'total_flos': 1004597150220288.0, 'train_loss': 1.2162665707998008, 'epoch': 3.0}) on AWS workspace desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f58b00-d0aa-4d1b-8bd5-8fed1a245ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./qwen2.5-finetuned\")  # Save the model\n",
    "tokenizer.save_pretrained(\"./qwen2.5-finetuned\")  # Save the tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
